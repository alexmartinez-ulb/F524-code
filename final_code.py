# -*- coding: utf-8 -*-
"""FINAL_CODE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lWGnHYpuWhNKlAyZh9DHTxO9jp6Z0xo1

FINAL CODE
"""

!pip install memory_profiler
!pip install seaborn

import numpy as np
import time
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import memory_profiler
from memory_profiler import memory_usage
import pandas as pd

##########################################################################PROXIMITY OPERATORS################################################

def soft_threshold(x, lambda_val):
    return np.sign(x) * np.maximum(np.abs(x) - lambda_val, 0)

def elastic_net_prox(x, lambda_l1, lambda_l2, L):
    return soft_threshold(x, lambda_l1 / L) / (1 + lambda_l2 / L)

def ridge_prox(x, lambda_l2, L):
    return x / (1 + lambda_l2 / L)

## Function to choose the right proximal operator in each case
def prox_operator(x, regularization_type, lambda_l1, lambda_l2, L):
    if regularization_type == 'l1':
        return soft_threshold(x, lambda_l1 / L)
    elif regularization_type == 'l2':
        return ridge_prox(x, lambda_l2, L)
    elif regularization_type == 'elastic_net':
        return elastic_net_prox(x, lambda_l1, lambda_l2, L)
    else:
        raise ValueError(f"Unknown regularization type: {regularization_type}")

######################################################LINE SEARCH METHODS###########################################################
def determine_reg_type(lambda_l1,lambda_l2):
     if lambda_l1 > 0 and lambda_l2 > 0:
        return 'elastic_net'
     elif lambda_l1 > 0:
        return 'l1'
     elif lambda_l2 > 0:
        return 'l2'

## Objective function

def objective_function(A, b, x, lambda_l1=0.0, lambda_l2=0.0, regularization_type='l1'):
    f_x = 0.5 * np.sum((A @ x - b) ** 2)
    if regularization_type == 'l1':
        g_x = lambda_l1 * np.sum(np.abs(x))
    elif regularization_type == 'l2':
        g_x = 0.5*lambda_l2 * np.sum(x ** 2)
    elif regularization_type == 'elastic_net':
        g_x = lambda_l1 * np.sum(np.abs(x)) + 0.5*lambda_l2 * np.sum(x ** 2)
    else:
        raise ValueError(f"Unknown regularization type: {regularization_type}")
    return f_x + g_x

def backtracking_line_search(A, b, x, grad, lambda_l1, lambda_l2, regularization_type, alpha=0.5, beta=0.8, max_iter=100):
    t = 1.0
    f_x = objective_function(A, b, x, lambda_l1, lambda_l2, regularization_type)
    grad_norm_sq = np.sum(grad ** 2)

    for _ in range(max_iter):
        x_new = prox_operator(x - t * grad, regularization_type, lambda_l1, lambda_l2, 1/t)
        f_x_new = objective_function(A, b, x_new, lambda_l1, lambda_l2, regularization_type)
        if f_x_new <= f_x - alpha * t * grad_norm_sq:
            return t
        t *= beta
    return t

############################################################ALGORITHMS###############################################################
## The way these functions are designed is explained in detail in the project report

def ista(A, b, lambda_l1, lambda_l2=0.0, max_iter=1000, tol=1e-6, return_iterations=False, regularization_type='elastic_net', line_search='fixed'):
    n_samples, n_features = A.shape
    x = np.zeros(n_features)
    if line_search == 'fixed':
      L = np.linalg.norm(A.T @ A, 2) + 2 * lambda_l2
    mse_history = []
    f_x_history=[]
    for k in range(max_iter):
        grad = A.T @ (A @ x - b) + 2 * lambda_l2 * x
        if line_search == 'fixed':
            x_new = prox_operator(x - grad / L, regularization_type, lambda_l1, lambda_l2, L)
        elif line_search == 'backtracking':
            t = backtracking_line_search(A, b, x, grad, lambda_l1, lambda_l2, regularization_type)
            x_new = prox_operator(x - t * grad, regularization_type, lambda_l1, lambda_l2, 1/t)
        else:
            raise ValueError(f"Unknown line search method: {line_search}")
        mse = np.mean((A @ x_new - b) ** 2)
        f_x = objective_function(A, b, x_new, lambda_l1, lambda_l2, regularization_type)
        mse_history.append(mse)
        f_x_history.append(f_x)
        if np.linalg.norm(x_new - x) < tol:
            break
        x = x_new

    if return_iterations:
        return x, k, mse_history, f_x_history
    return x

def fista(A, b, lambda_l1, lambda_l2=0.0, max_iter=1000, tol=1e-6, return_iterations=False, regularization_type='elastic_net', line_search='fixed'):
    n_samples, n_features = A.shape
    x = np.zeros(n_features)
    y = x.copy()
    t = 1
    if line_search == 'fixed':
     L = np.linalg.norm(A.T @ A, 2) + 2 * lambda_l2
    mse_history = []
    f_x_history=[]
    for k in range(max_iter):
        grad = A.T @ (A @ y - b) + 2 * lambda_l2 * y

        if line_search == 'fixed':
            x_new = prox_operator(y - grad / L, regularization_type, lambda_l1, lambda_l2, L)
        elif line_search == 'backtracking':
            t_step = backtracking_line_search(A, b, y, grad, lambda_l1, lambda_l2, regularization_type)
            x_new = prox_operator(y - t_step * grad, regularization_type, lambda_l1, lambda_l2, 1/t_step)
        else:
            raise ValueError(f"Unknown line search method: {line_search}")

        t_new = (1 + np.sqrt(1 + 4 * t**2)) / 2
        y = x_new + ((t - 1) / t_new) * (x_new - x)

        mse = np.mean((A @ x_new - b) ** 2)
        mse_history.append(mse)
        f_x = objective_function(A, b, x_new, lambda_l1, lambda_l2, regularization_type)
        f_x_history.append(f_x)
        if np.linalg.norm(x_new - x) < tol:
            break

        x, t = x_new, t_new

    if return_iterations:
        return x, k, mse_history, f_x_history
    return x


def gradient_descent(A, b, lambda_l2=0.0, max_iter=1000, tol=1e-6, return_iterations=True, line_search='fixed'):
    n_features = A.shape[1]
    x = np.zeros(n_features)
    mse_history = []
    f_x_history=[]
    if line_search == 'fixed':
        L = np.linalg.norm(A.T @ A, 2) + 2 * lambda_l2
        lr = 1 / L

    for k in range(int(max_iter)):
        grad = A.T @ (A @ x - b) + 2 * lambda_l2 * x

        if line_search == 'fixed':
            x_new = x - lr * grad
        elif line_search == 'backtracking':
            t = backtracking_line_search(A, b, x, grad, 0.0, lambda_l2, 'l2')
            x_new = x - t * grad
        else:
            raise ValueError(f"Unknown line search method: {line_search}")

        mse = np.mean((A @ x_new - b) ** 2)
        mse_history.append(mse)
        f_x = objective_function(A, b, x_new,lambda_l2,0, regularization_type=determine_reg_type(0,lambda_l2))
        f_x_history.append(f_x)
        if np.linalg.norm(x_new - x) < tol:
            break
        x = x_new

    if return_iterations:
        return x, k, mse_history, f_x_history
    return x

def dual_fista(A, b, lambda_l1, lambda_l2=0, max_iter=1000, tol=1e-6, return_iterations=False):
    """
    Dual-FISTA solver for LASSO problems (ℓ₁ only).

    Solves:
        min_x 0.5 * ||Ax - b||^2 + lambda_l1 * ||x||_1

    by solving its dual:
        max_alpha -0.5 * ||alpha||^2 - b^T alpha
        subject to ||A^T alpha||_∞ ≤ lambda_l1

    Parameters:
        A : ndarray, shape (n_samples, n_features)
        b : ndarray, shape (n_samples,)
        lambda_l1 : regularization parameter (ℓ₁)
        max_iter : maximum number of iterations
        tol : convergence tolerance
        return_iterations : if True, returns diagnostics

    Returns:
        x : primal solution
        (optionally: k, mse_history, f_x_history)
    """
    n_samples, _ = A.shape
    alpha = np.zeros(n_samples)
    y = alpha.copy()
    t = 1.0
    L = np.linalg.norm(A @ A.T, 2)
    mse_history = []
    f_x_history = []

    for k in range(max_iter):
        grad = A @ (A.T @ y) - b
        alpha_new = y - grad / L

        # Project onto dual feasible set: ||Aᵗ alpha||_∞ ≤ lambda₁
        AT_alpha = A.T @ alpha_new
        scaling = max(1.0, np.max(np.abs(AT_alpha)) / lambda_l1)
        alpha_new /= scaling

        # FISTA update
        t_new = (1 + np.sqrt(1 + 4 * t**2)) / 2
        y = alpha_new + ((t - 1) / t_new) * (alpha_new - alpha)
        alpha = alpha_new
        t = t_new

        # Primal estimate
        x_est = A.T @ alpha
        mse = np.mean((A @ x_est - b) ** 2)
        mse_history.append(mse)

        # Objective
        f_x = 0.5 * np.linalg.norm(A @ x_est - b) ** 2 + lambda_l1 * np.linalg.norm(x_est, 1)
        f_x_history.append(f_x)

        if np.linalg.norm(grad) < tol:
            break

    if return_iterations:
        return x_est, k, mse_history, f_x_history
    return x_est

"""This benchmarking functions enables us to solve the regression problem for each algortihm, for a wide range of parameters lambda_l1, lambda_l2, max iter, tolerance ....

We then get as an outpout a dataframe containing the average of the n_runs from each algorithm of most of the essential performace measures. It also gives us the value of the MSE and the objective function at each step during each run.

It can be run on synthetic datasets to observe if the expected behavior is followed or on real datasets to make sure the theoritical promises are fufilled.
"""

######################################################## BENCHMARKING ##############################################################

def run_benchmark(algorithms, data_configs, lambda_range_l1, lambda_range_l2=None, max_iter=1000, tol=1e-3,
                  return_iterations=True, line_search='fixed', n_runs=1,
                  real_datasets=None, target_cols=None):
    if lambda_range_l2 is None:
        lambda_range_l2 = [0.0]
    results = []
    records = []
    # Takes the real dataset and extracts the target column, the splits the samples into training and testing to
    # Make sure our algorithm finds a suitable solution
    if real_datasets:
        if target_cols is None:
            raise ValueError("target_cols must be specified when using real_datasets.")
        dataset_iter = []
        for name, df in real_datasets.items():
            if name not in target_cols:
                raise ValueError(f"Missing target column for dataset '{name}' in target_cols.")
            target_col = target_cols[name]
            A = df.drop(columns=[target_col]).values
            b = df[target_col].values
            from sklearn.model_selection import train_test_split
            A_train, A_test, b_train, b_test = train_test_split(A, b, test_size=0.2, random_state=42)
            dataset_iter.append((name, A_train, b_train, A_test, b_test, None))
    else:
        dataset_iter = []
        for i, config in enumerate(data_configs):
            A, b, x_true = generate_data(**config)
            dataset_iter.append((f"synthetic_{i}", A, b, None, None, x_true))

    for dataset in dataset_iter:
        dataset_name, A, b, A_test, b_test, x_true = dataset

        for lambda_l1 in lambda_range_l1:
            for lambda_l2 in lambda_range_l2:
                reg_type = 'elastic_net' if lambda_l1 > 0 and lambda_l2 > 0 else 'l1' if lambda_l1 > 0 else 'l2'
                for algo_name in algorithms:
                    algo_func = globals()[algo_name]
                    exec_times = []
                    iterations = []
                    memories = []
                    solutions = []
                    last_mse_hist = []

                    for _ in range(n_runs):
                        start_time = time.time()

                        if algo_name in ['ista', 'fista']:
                            mem_usage = memory_usage((algo_func, (A, b, lambda_l1), {
                                'lambda_l2': lambda_l2,
                                'max_iter': max_iter,
                                'tol': tol,
                                'return_iterations': return_iterations,
                                'regularization_type': reg_type,
                                'line_search': line_search
                            }), interval=0.1, timeout=1)
                            x, iters, mse_hist ,f_x_hist = algo_func(A, b, lambda_l1, lambda_l2, max_iter, tol,
                                                          return_iterations, reg_type, line_search)
                        elif algo_name == 'dual_fista':
                            mem_usage = memory_usage((algo_func, (A, b, lambda_l1), {
                                'lambda_l2': lambda_l2,
                                'max_iter': max_iter,
                                'tol': tol,
                                'return_iterations': return_iterations
                            }), interval=0.1, timeout=1)
                            x, iters, mse_hist, f_x_hist = algo_func(A, b, lambda_l1, lambda_l2, max_iter, tol,
                                                          return_iterations)
                        elif algo_name == 'gradient_descent':
                            mem_usage = memory_usage((algo_func, (A, b), {
                                'lambda_l2': lambda_l2,
                                'max_iter': max_iter,
                                'tol': tol,
                                'return_iterations': return_iterations,
                                'line_search': line_search
                            }), interval=0.1, timeout=1)
                            x, iters, mse_hist, f_x_hist = algo_func(A, b, lambda_l2=lambda_l2, max_iter=max_iter,
                                                          tol=tol, return_iterations=return_iterations,
                                                          line_search=line_search)
                        else:
                            mem_usage = memory_usage((algo_func, (A, b, lambda_l1, lambda_l2), {
                                'return_iterations': return_iterations
                            }), interval=0.1, timeout=1)
                            x, iters = algo_func(A, b, lambda_l1, lambda_l2, return_iterations)
                            mse_hist = []

                        end_time = time.time()
                        exec_times.append(end_time - start_time)
                        iterations.append(iters)
                        memories.append(max(mem_usage))
                        solutions.append(x)
                        last_mse_hist = mse_hist
                        last_fx_hist= f_x_hist

                    avg_solution = np.mean(solutions, axis=0)
                    mse_train = np.mean((A @ avg_solution - b) ** 2)
                    mse_test = np.mean((A_test @ avg_solution - b_test) ** 2) if A_test is not None else np.nan
                    sparsity = np.mean(np.abs(avg_solution) < tol)
                    error_to_true = np.nan
                    if real_datasets is None:
                        error_to_true = np.linalg.norm(avg_solution - x_true)

                    results.append({
                        'dataset': dataset_name,
                        'algorithm': algo_name,
                        'lambda_l1': lambda_l1,
                        'lambda_l2': lambda_l2,
                        'n_samples': A.shape[0],
                        'n_features': A.shape[1],
                        'sparsity': sparsity,
                        'avg_time': np.mean(exec_times),
                        'std_time': np.std(exec_times),
                        'min_time': np.min(exec_times),
                        'max_time': np.max(exec_times),
                        'max_iterations': np.nanmax(iterations),
                        'min_iterations': np.nanmin(iterations),
                        'avg_iterations': np.nanmean(iterations),
                        'std_iterations': np.nanstd(iterations),
                        'avg_memory': np.mean(memories),
                        'mse_train': mse_train,
                        'mse_test': mse_test,
                        'solution_sparsity': sparsity,
                        'error_to_true': error_to_true
                    })

                    if last_mse_hist:
                        records.append({ 'algorithm': algo_name,
                                              'dataset': dataset_name,
                                             'lambda_l1': lambda_l1,
                                             'lambda_l2': lambda_l2,
                                             'mse_history': last_mse_hist,
                                             'f_x_history': last_fx_hist
                                      })

    return pd.DataFrame(results), records

"""We used google colab for the project so the datatsets were stored in the Google Drive"""

from google.colab import drive
drive.mount('/content/drive')

"""Here we import the 2 datasets and define the target columns.

These datasets have already been preprocessed for the most part. Except the value of of the Sale prices for the House prices dataset were we applied a log function to make their values more manageable.
"""

real_datasets = {
    "house_prices": pd.read_csv("/content/drive/MyDrive/house_prices_preprocessed.csv"),
    "Student_grades" : pd.read_csv("/content/drive/MyDrive/students_normalized2.csv")
}
target_cols = {
    "house_prices": "SalePrice",
    "Student_grades" : "math score"
}
real_datasets["house_prices"]["SalePrice"] = np.log1p(real_datasets["house_prices"]["SalePrice"])

"""Glimpse of the different datsets; the design matrix of each dataset is very different in number of features (288 vs 14). And the values of said features are also very different with the Student Grades matrix being quite sparse."""

real_datasets["house_prices"].head(5)

real_datasets["Student_grades"].head(5)

def get_best_hyperparams(results_df, metric="mse_test"):
    """
    Extract the best (lambda_l1, lambda_l2) combination for each algorithm and dataset
    based on the lowest mse_test (or another metric).

    Parameters:
    - results_df: DataFrame returned by run_benchmark
    - metric: the metric to minimize (default: mse_test)

    Returns:
    - DataFrame with one row per (algorithm, dataset) with the best hyperparameters
    """
    # Filter out NaNs just in case
    filtered_df = results_df.dropna(subset=[metric])

    # Group by algorithm and dataset, select the row with minimal metric value
    best_params = (
        filtered_df.loc[filtered_df.groupby(["algorithm", "dataset"])[metric].idxmin()]
        .reset_index(drop=True)
        .sort_values(by=["dataset", "algorithm"])
    )

    return best_params

"""  This is the grid in which we will be looking for the best pair of lamabda_l1 and lambda_l2 for each algorithm.


  This is essential to give a fair shot to each algorithm on each dataset, if we do not complete this step and compared the algorithms for the same parameters then we would just see which algorithm is best for each set of parameters and not the one who is truly superior in a real life application.
"""

range_l1= [10e-5, 10e-4, 10e-3, 10e-2, 10e-1, 1]
range_l2= [10e-5, 10e-4, 10e-3, 10e-2, 10e-1, 1]

results_ista, records_ista = run_benchmark(
    algorithms=['ista'],
    data_configs=[],  # real dataset so empty
    lambda_range_l1=range_l1,
    lambda_range_l2=range_l2,
    real_datasets=real_datasets,
    target_cols=target_cols,
    max_iter=2000,
    tol=1e-5,
    return_iterations=True,
    line_search='fixed',
    n_runs=3
)
best_params_ista = get_best_hyperparams(results_ista)
print(best_params_ista)

results_fista, records_fista = run_benchmark(
    algorithms=['fista'],
    data_configs=[],  # real datasets so empty
    lambda_range_l1=range_l1,
    lambda_range_l2=range_l2,
    real_datasets=real_datasets,
    target_cols=target_cols,
    max_iter=2000,
    tol=1e-5,
    return_iterations=True,
    line_search='fixed',
    n_runs=3
)
best_params_fista = get_best_hyperparams(results_fista)
print(best_params_fista)

results_GD, records__GD = run_benchmark(
    algorithms=['gradient_descent'],
    data_configs=[],  # real datasets so empty
    lambda_range_l1=[0],
    lambda_range_l2=range_l2,
    real_datasets=real_datasets,
    target_cols=target_cols,
    max_iter=2000,
    tol=1e-5,
    return_iterations=True,
    line_search='fixed',
    n_runs=3
)
best_params_GD = get_best_hyperparams(results_GD)
print(best_params_GD)

results_dual, records_dual = run_benchmark(
    algorithms=['dual_fista'],
    data_configs=[],  # real datasets so empty
    lambda_range_l1=range_l1,
    lambda_range_l2=[0],
    real_datasets=real_datasets,
    target_cols=target_cols,
    max_iter=2000,
    tol=1e-5,
    return_iterations=True,
    line_search='fixed',
    n_runs=3
)
best_params_dual = get_best_hyperparams(results_dual)
print(best_params_dual)

"""Combining the best parameters and the value of the performance indicators for each algorithm"""

def merge_best_param_tables(*tables):
    """
    Concatenate multiple best_param DataFrames into a single combined DataFrame.

    Parameters:
    - *tables: any number of DataFrames to merge

    Returns:
    - A single concatenated DataFrame
    """
    merged_df = pd.concat(tables, axis=0, ignore_index=True)
    return merged_df

combined_best = merge_best_param_tables(best_params_ista, best_params_fista, best_params_dual, best_params_GD)
print(combined_best)

"""Combining the different records to be able to plot them afterwards"""

def flatten_record_lists(*record_lists):
    """
    Flatten multiple lists of benchmark records into a single list.

    Parameters:
    - *record_lists: one or more lists of benchmark records (dicts)

    Returns:
    - A single flat list combining all records
    """
    flat_records = []
    for record_list in record_lists:
        flat_records.extend(record_list)
    return flat_records
combined_records = flatten_record_lists(records_ista, records_fista, records_dual, records__GD)
print(combined_records)

"""Function to plot the MSE of each algorithm as a function of iterations, we use the best hyperparameters for each algorithm of course.

"""

def plot_best_mse(records, best_params_df, dataset_name):
    """
    Plot MSE vs Iteration for all algorithms on the same dataset using their best hyperparameters.

    Parameters:
    - records: list of benchmark record dicts
    - best_params_df: DataFrame containing best parameters for each (algorithm, dataset)
    - dataset_name: name of the dataset to filter
    """
    plt.figure(figsize=(10, 6))
    found_any = False

    for _, row in best_params_df.iterrows():
        if row['dataset'] != dataset_name:
            continue

        algo = row['algorithm']
        l1 = row['lambda_l1']
        l2 = row['lambda_l2']

        for rec in records:
            if (rec['dataset'] == dataset_name and rec['algorithm'] == algo and
                rec['lambda_l1'] == l1 and rec['lambda_l2'] == l2):
                plt.plot(rec['mse_history'], label=f"{algo} (λ₁={l1}, λ₂={l2})")
                found_any = True
                break

    if found_any:
        plt.title(f"MSE vs Iterations for All Algorithms on {dataset_name}")
        plt.xlabel("Iteration")
        plt.ylabel("MSE")
        plt.yscale('log')
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()
    else:
        print(f"No records found for dataset: {dataset_name}")

plot_best_mse(combined_records, combined_best, "house_prices")

plot_best_mse(combined_records, combined_best, "Student_grades")

"""Function to plot the evolution of the objective function for each algorithm at the best hyperparameters"""

def plot_best_fx(records, best_params_df, dataset_name):
    """
    Plot MSE vs Iteration for all algorithms on the same dataset using their best hyperparameters.

    Parameters:
    - records: list of benchmark record dicts
    - best_params_df: DataFrame containing best parameters for each (algorithm, dataset)
    - dataset_name: name of the dataset to filter
    """
    plt.figure(figsize=(10, 6))
    found_any = False

    for _, row in best_params_df.iterrows():
        if row['dataset'] != dataset_name:
            continue

        algo = row['algorithm']
        l1 = row['lambda_l1']
        l2 = row['lambda_l2']

        for rec in records:
            if (rec['dataset'] == dataset_name and rec['algorithm'] == algo and
                rec['lambda_l1'] == l1 and rec['lambda_l2'] == l2):
                plt.plot(rec['f_x_history'], label=f"{algo} (λ₁={l1}, λ₂={l2})")
                found_any = True
                break

    if found_any:
        plt.title(f"f(x) vs Iterations for All Algorithms on {dataset_name}")
        plt.xlabel("Iteration")
        plt.ylabel("f(x)")
        plt.yscale('log')
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()
    else:
        print(f"No records found for dataset: {dataset_name}")

plot_best_fx(combined_records, combined_best, "house_prices")

plot_best_fx(combined_records, combined_best, "Student_grades")

"""Creating representations of the different performance metrics of the best algorithms."""

# Group by algorithm and compute mean execution time
avg_times = combined_best.groupby(['dataset','algorithm'])['avg_time'].mean().sort_values()

# Plotting
plt.figure(figsize=(10, 6))
avg_times.plot(kind='barh')
plt.xlabel('Average Execution Time (s)')
plt.title('Benchmark: Average Execution Time per Algorithm')
plt.grid(True)
plt.tight_layout()
plt.show()

# Plot: Average Number of Iterations per Algorithm
avg_iterations = combined_best.groupby(['dataset','algorithm'])['avg_iterations'].mean().sort_values()

plt.figure(figsize=(10, 6))
avg_iterations.plot(kind='barh')
plt.xlabel('Average Number of Iterations')
plt.title('Benchmark: Average Number of Iterations per Algorithm')
plt.grid(True)
plt.tight_layout()
plt.show()

# Plot: Average Sparsity per Algorithm
avg_sparsity = combined_best.groupby(['dataset','algorithm'])['solution_sparsity'].mean().sort_values()

plt.figure(figsize=(10, 6))
avg_sparsity.plot(kind='barh')
plt.xlabel('Average Sparsity (Fraction of Zero Coefficients)')
plt.title('Benchmark: Solution Sparsity per Algorithm')
plt.grid(True)
plt.tight_layout()
plt.show()

# Plot: Average Memory per Algorithm
avg_sparsity = combined_best.groupby(['dataset','algorithm'])['avg_memory'].mean().sort_values()

plt.figure(figsize=(10, 6))
avg_sparsity.plot(kind='barh')
plt.xlabel('Average Memory used')
plt.title('Benchmark: Memory used to find the solution')
plt.grid(True)
plt.tight_layout()
plt.show()

# Plot: Average MSE train
avg_mse_train = combined_best.groupby(['dataset', 'algorithm'])['mse_train'].mean().sort_values()

plt.figure(figsize=(10, 6))
avg_mse_train.plot(kind='barh')
plt.xlabel('Average Train MSE')
plt.title('Benchmark: Average Train MSE per Algorithm and Dataset')
plt.grid(True)
plt.tight_layout()
plt.show()

# Plot: Average MSE test
avg_mse_test = combined_best.groupby(['dataset', 'algorithm'])['mse_test'].mean().sort_values()

plt.figure(figsize=(10, 6))
avg_mse_test.plot(kind='barh')
plt.xlabel('Average Test MSE')
plt.title('Benchmark: Average Test MSE per Algorithm and Dataset')
plt.grid(True)
plt.tight_layout()
plt.show()

results_GD_back, records__GD_back = run_benchmark(
    algorithms=['gradient_descent'],
    data_configs=[],  # real datasets so empty
    lambda_range_l1=[0],
    lambda_range_l2=range_l2,
    real_datasets=real_datasets,
    target_cols=target_cols,
    max_iter=2000,
    tol=1e-5,
    return_iterations=True,
    line_search='backtracking',
    n_runs=1
)
best_params_GD_back = get_best_hyperparams(results_GD_back)
print(best_params_GD_back)

"""Quick test of backtracking on gradient descent, we can see that it makes the computing time explode with a very small improvement of the MSE. We multiply computing time by 30 and get an improvement of about 50% for this reason we will not be using backtracking for gradient descent."""